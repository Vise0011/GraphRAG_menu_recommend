import torch
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

MODEL_DIR = "/root/16_team/app/llama/Llama-3.1-8B-Instruct"

_model = None
_tokenizer = None

def load_model():
    global _model, _tokenizer
    if _model is not None: return _model, _tokenizer

    print("â³ ëª¨ë¸ ë¡œë”© ì¤‘...")
    gc.collect()
    torch.cuda.empty_cache()
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    _tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    _model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    # Llama-3 íŒ¨ë”© í† í° ì„¤ì •
    if _tokenizer.pad_token_id is None:
        _tokenizer.pad_token_id = _tokenizer.eos_token_id
        
    return _model, _tokenizer

class PromptRequest(BaseModel):
    top5: list

@app.post("/prompt")
async def generate_prompt(req: PromptRequest):
    return {"result": ask_hf_llama(req.top5)}

def ask_hf_llama(top5_list: list[dict], conditions: dict = None) -> str:
    model, tokenizer = load_model()

    menu_names = [item.get("menu", "") for item in top5_list]
    rec_menu_str = ", ".join(menu_names)
    target_menu = menu_names[0] if menu_names else "ì¶”ì²œ ë©”ë‰´"

    # ====================================================
    # ğŸ¯ 1. ë¡œì§ ë¶„ê¸° (ìƒí™© vs ìœ ì €) - ë¬¸ë§¥(Context) ìƒì„±
    # ====================================================
    if conditions and conditions.get("logic") == "User Similarity":
        # [2ë²ˆ] ë¹„ìŠ·í•œ ìœ ì € Pick
        history = conditions.get("history", "ì´ì „ ë©”ë‰´")
        context_desc = f"ì‚¬ìš©ìëŠ” ê³¼ê±°ì— '{history}'ë¥¼ ì£¼ë¬¸í–ˆìŒ. ìœ ì‚¬í•œ ì…ë§›ì˜ ê·¸ë£¹ì€ '{target_menu}'ë¥¼ ì„ í˜¸í•¨."
        
        guide_sentence = (
            f"ì†ë‹˜, ì´ì „ì— {history}ë¥¼ ë§›ìˆê²Œ ë“œì…¨êµ°ìš”! "
            f"íšŒì›ë‹˜ê³¼ ì…ë§›ì´ ê¼­ ë‹®ì€ ë¯¸ì‹ê°€ë¶„ë“¤ì€ ì£¼ë¡œ {target_menu}ë¥¼ ì„ íƒí•˜ì…¨ì–´ìš”. "
            f"ì´ ë©”ë‰´ëŠ” [ë§›/ì‹ê° íŠ¹ì§•]ì´ ìˆì–´ì„œ íšŒì›ë‹˜ ì·¨í–¥ì„ ì €ê²©í•  ê±°ì˜ˆìš”!"
        )
    
    elif conditions:
        # [1ë²ˆ] ìƒí™© ê¸°ë°˜ ì¶”ì²œ
        cond_list = []
        if conditions.get('people'): cond_list.append(f"ì¸ì› {conditions['people']}")
        if conditions.get('rain') and conditions['rain'] not in ['ì—†ìŒ', '0mm']: 
            cond_list.append(f"ë‚ ì”¨ {conditions['rain']} ë¹„")
        elif conditions.get('season'):
            cond_list.append(f"ê³„ì ˆ {conditions['season']}")
        if conditions.get('time'): cond_list.append(f"ì‹œê°„ {conditions['time']}")
        
        price = conditions.get('price', '0')
        if price in ['0', '0~10000ì›']: price_desc = "ê°€ì„±ë¹„ ì˜ˆì‚°"
        else: price_desc = f"ì˜ˆì‚° {price}"
        cond_list.append(price_desc)

        if conditions.get('category'): cond_list.append(f"ì„ í˜¸ ì¹´í…Œê³ ë¦¬ {conditions['category']}")

        situation_summary = ", ".join(cond_list)
        context_desc = f"í˜„ì¬ ìƒí™©: {situation_summary}. ì¶”ì²œ ë©”ë‰´: {target_menu}."
        
        guide_sentence = (
            f"ì†ë‹˜, í˜„ì¬ {situation_summary}ì¸ ìƒí™©ì— ë§ì¶°, "
            f"ë‹¤ë¥¸ ì†ë‹˜ë“¤ì´ ê°€ì¥ ë§ì´ ì°¾ìœ¼ì‹  {target_menu}ë¥¼ ê°•ë ¥ ì¶”ì²œë“œë ¤ìš”! "
            f"ì´ ë©”ë‰´ëŠ” [ë§›/ì‹ê° íŠ¹ì§•]ì´ ìˆì–´ì„œ ì§€ê¸ˆ ìƒí™©ì— ë”±ì…ë‹ˆë‹¤."
        )

    else:
        # ê¸°ë³¸
        context_desc = f"ì¼ë°˜ ì¶”ì²œ ìƒí™©. ë©”ë‰´: {target_menu}"
        guide_sentence = f"ì†ë‹˜, ìš”ì¦˜ ì œì¼ ì˜ ë‚˜ê°€ëŠ” {target_menu}ë¥¼ ì¶”ì²œë“œë ¤ìš”!"

    # ====================================================
    # ğŸ“ 2. Llama-3 ì „ìš© Chat í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í•µì‹¬ ìˆ˜ì •)
    # ====================================================
    
    # System Message: ì—­í•  ë¶€ì—¬
    system_prompt = (
        "ë„ˆëŠ” ì´ìì¹´ì•¼ì˜ ì¹œì ˆí•˜ê³  ì„¼ìŠ¤ ìˆëŠ” ì ì¥ì´ë‹¤. "
        "ì£¼ì–´ì§„ ìƒí™©ê³¼ ë©”ë‰´ì— ëŒ€í•´ ì†ë‹˜ì—ê²Œ ê¶Œí•˜ëŠ” ë§ì„ í•œ ë§ˆë””ë¡œ ì‘ì„±í•´ë¼. "
        "ì„¤ëª…ì€ êµ¬ì²´ì ì´ê³  ê°ê°ì ì´ì–´ì•¼ í•˜ë©°(3ë¬¸ì¥ ì´ìƒ), ì—†ëŠ” ì¬ë£Œë¥¼ ì§€ì–´ë‚´ë©´ ì•ˆ ëœë‹¤."
    )

    # User Message: ì…ë ¥ ë°ì´í„°ì™€ ì§€ì‹œì‚¬í•­
    user_prompt = f"""
    [ìƒí™© ì •ë³´]
    {context_desc}

    [ë‹µë³€ ê°€ì´ë“œë¼ì¸]
    ë‹¤ìŒ ë¬¸ì¥ íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì™„ì„±í•´ë¼:
    "{guide_sentence}"

    [ì£¼ì˜ì‚¬í•­]
    1. ê°€ì´ë“œë¼ì¸ì˜ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•˜ë˜, ë’¤ì— ë©”ë‰´ì˜ ë§›ê³¼ ì‹ê°ì„ ì•„ì£¼ í’ì„±í•˜ê²Œ ë¬˜ì‚¬í•´ë¼.
    2. 'ë‹µì•ˆ:', 'ì ì¥:', 'ì£¼ì˜:' ê°™ì€ í—¤ë”ë¥¼ ì ˆëŒ€ ë¶™ì´ì§€ ë§ˆë¼.
    3. ì˜¤ì§ ì ì¥ì˜ ëŒ€ì‚¬ë§Œ ì¶œë ¥í•´ë¼.
    """

    # ğŸ”¥ Llama-3 Chat Template ì ìš©
    # <|begin_of_text|>...<|start_header_id|>assistant<|end_header_id|>
    prompt = (
        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
        f"<|start_header_id|>user<|end_header_id|>\n\n{user_prompt}<|eot_id|>"
        f"<|start_header_id|>assistant<|end_header_id|>\n\n"
        f"ì ì¥: ì†ë‹˜," # ğŸ‘ˆ AIê°€ ì—¬ê¸°ì„œë¶€í„° ë§í•˜ë„ë¡ ê°•ì œ ì‹œì‘ì  ìƒì„±
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=400,
            do_sample=True,
            top_p=0.9,
            temperature=0.4, 
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id
        )

    # ====================================================
    # ğŸ§¹ 3. í›„ì²˜ë¦¬ (Cleaning)
    # ====================================================
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # "ì ì¥: ì†ë‹˜," ë’·ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê¸°
    if "ì ì¥: ì†ë‹˜," in full_text:
        # promptì— ë„£ì—ˆë˜ ì‹œì‘ì  ë’¤ì— AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì„
        generated_part = full_text.split("ì ì¥: ì†ë‹˜,")[-1].strip()
        final_response = "ì†ë‹˜, " + generated_part
    else:
        # í˜¹ì‹œë¼ë„ í¬ë§·ì´ ê¹¨ì§€ë©´ í”„ë¡¬í”„íŠ¸ ì œê±° í›„ ì‚¬ìš©
        final_response = full_text.replace(prompt, "").strip()

    # ì¡ë‹¤í•œ ê¸°í˜¸ ì œê±°
    garbage = ["[ë‹µì•ˆ]", "ë‹µ:", "*ì£¼ì˜*", "Note:", "ë¹„ê³ :", "ì‹œìŠ¤í…œ:", "user:", "assistant:"]
    for g in garbage:
        final_response = final_response.replace(g, "")
        
    return final_response.strip()

# í˜¸í™˜ì„± ìœ ì§€
def ask_site2_llama(top5_list, base_menu=None):
    return ask_hf_llama(top5_list)